{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrbPoz6vTLvUaP306kAEF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepgundeboina/SmallApps/blob/main/ExtractYouTubeTitles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOBfS5fn2vB9",
        "outputId": "36845e86-d02f-4d3e-d9d2-f27b11b493ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.6.9-py3-none-any.whl.metadata (174 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/174.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/174.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.6.9-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.6.9\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import json\n",
        "import sys\n",
        "\n",
        "def get_playlist_titles(playlist_url):\n",
        "    \"\"\"\n",
        "    Extracts titles from a YouTube playlist using yt-dlp.\n",
        "\n",
        "    Args:\n",
        "        playlist_url (str): The URL of the YouTube playlist.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of video titles from the playlist.\n",
        "              Returns an empty list if there's an error or no videos found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Command to run yt-dlp:\n",
        "        # --flat-playlist: Don't extract the videos, just the playlist information.\n",
        "        # --dump-json: Output the information as JSON.\n",
        "        # --no-warnings: Suppress yt-dlp warnings.\n",
        "        command = [\n",
        "            sys.executable,  # Use the current Python interpreter\n",
        "            '-m', 'yt_dlp',\n",
        "            '--flat-playlist',\n",
        "            '--dump-json',\n",
        "            '--no-warnings',\n",
        "            playlist_url\n",
        "        ]\n",
        "\n",
        "        print(f\"Fetching titles from: {playlist_url}\")\n",
        "        print(\"This might take a moment for large playlists...\")\n",
        "\n",
        "        # Execute the command and capture stdout\n",
        "        process = subprocess.run(\n",
        "            command,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,  # Raise an exception for non-zero exit codes\n",
        "            encoding='utf-8' # Explicitly set encoding to avoid issues with special characters\n",
        "        )\n",
        "\n",
        "        # Each line in the output is a JSON object for a video\n",
        "        titles = []\n",
        "        for line in process.stdout.strip().split('\\n'):\n",
        "            if line:\n",
        "                try:\n",
        "                    video_info = json.loads(line)\n",
        "                    if 'title' in video_info:\n",
        "                        titles.append(video_info['title'])\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Could not decode JSON from line: {line[:100]}...\") # Print a snippet\n",
        "                    continue\n",
        "        return titles\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running yt-dlp: {e}\")\n",
        "        print(f\"Stderr: {e.stderr}\")\n",
        "        return []\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'yt-dlp' or 'python' command not found.\")\n",
        "        print(\"Please ensure yt-dlp is installed (`pip install yt-dlp`)\")\n",
        "        print(\"and that Python is correctly configured in your system's PATH.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # You can uncomment and use your own playlist URL here:\n",
        "    playlist_url = input(\"Enter the YouTube playlist URL: \")\n",
        "\n",
        "    if not playlist_url:\n",
        "        print(\"No playlist URL provided. Exiting.\")\n",
        "    else:\n",
        "        video_titles = get_playlist_titles(playlist_url)\n",
        "\n",
        "        if video_titles:\n",
        "            print(\"\\n--- Extracted Titles ---\")\n",
        "            for i, title in enumerate(video_titles, 1):\n",
        "                print(f\"{i}. {title}\")\n",
        "\n",
        "            # Optional: Save titles to a text file\n",
        "            save_to_file = input(\"\\nDo you want to save these titles to a text file? (yes/no): \").lower()\n",
        "            if save_to_file == 'yes':\n",
        "                file_name = input(\"Enter the filename (e.g., my_playlist_titles.txt): \")\n",
        "                if not file_name:\n",
        "                    file_name = \"playlist_titles.txt\"\n",
        "                try:\n",
        "                    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "                        for title in video_titles:\n",
        "                            f.write(title + '\\n')\n",
        "                    print(f\"Titles saved to '{file_name}' successfully!\")\n",
        "                except IOError as e:\n",
        "                    print(f\"Error saving to file: {e}\")\n",
        "        else:\n",
        "            print(\"Could not extract any titles from the playlist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpV7AgEt2yaY",
        "outputId": "adce0b68-ced6-4f88-ef5e-ccff36736ad1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube playlist URL: https://www.youtube.com/watch?v=4JP0XqsjwCI&list=PLgPb8HXOGtsQeiFz1y9dcLuXjRh8teQtw\n",
            "Fetching titles from: https://www.youtube.com/watch?v=4JP0XqsjwCI&list=PLgPb8HXOGtsQeiFz1y9dcLuXjRh8teQtw\n",
            "This might take a moment for large playlists...\n",
            "\n",
            "--- Extracted Titles ---\n",
            "1. 01. Databricks: Spark Architecture & Internal Working Mechanism\n",
            "2. 02. Databricks | PySpark: RDD, Dataframe and Dataset\n",
            "3. 03. Databricks | PySpark: Transformation and Action\n",
            "4. 04. On-Heap vs Off-Heap| Databricks | Spark | Interview Question | Performance Tuning\n",
            "5. 05. Databricks | Pyspark: Cluster Deployment\n",
            "6. 06. Databricks | Pyspark| Spark Reader: Read CSV File\n",
            "7. 07. Databricks | Pyspark:  Filter Condition\n",
            "8. 08. Databricks | Pyspark: Add, Rename and Drop Columns\n",
            "9. 09. Databricks  | PySpark Join Types\n",
            "10. 10. Databricks | Pyspark:  Utility Commands - DBUtils\n",
            "11. 11. Databricks | Pyspark: Explode Function\n",
            "12. 12. Databricks | Pyspark: Case Function (When.Otherwise )\n",
            "13. 13. Databricks | Pyspark: Union & UnionAll\n",
            "14. 14. Databricks | Pyspark: Pivot & Unpivot\n",
            "15. 15. Databricks| Spark | Pyspark | Read Json| Flatten Json\n",
            "16. 16. Databricks | Spark | Pyspark | Bad Records Handling | Permissive;DropMalformed;FailFast\n",
            "17. 17. Databricks & Pyspark: Azure Data Lake Storage Integration with Databricks\n",
            "18. 18. Databricks & Pyspark: Ingest Data from Azure SQL Database\n",
            "19. 19. Databricks & Pyspark: Real Time ETL Pipeline Azure SQL to ADLS\n",
            "20. 20. Databricks & Pyspark: Azure Key Vault Integration\n",
            "21. 21. Databricks| Spark Streaming\n",
            "22. 22. Databricks| Spark | Performance Optimization | Repartition vs Coalesce\n",
            "23. 23. Databricks | Spark | Cache vs Persist | Interview Question | Performance Tuning\n",
            "24. 24. Databricks| Spark | Interview Questions| Catalyst Optimizer\n",
            "25. 25. Databricks | Spark | Broadcast Variable| Interview Question | Performance Tuning\n",
            "26. 26. Databricks | Spark | Adaptive Query Execution| Interview Question | Performance Tuning\n",
            "27. 31. Databricks Pyspark: Handling Null - Part1\n",
            "28. 32. Databricks| Pyspark| Handling Null Part 2\n",
            "29. 33. Databricks | Spark | Pyspark | UDF\n",
            "30. 34. Databricks - Spark: Data Skew Optimization\n",
            "31. 35. Databricks & Spark: Interview Question - Shuffle Partition\n",
            "32. 36. Databricks: Autoscaling | Optimized Autoscaling\n",
            "33. 37. Databricks | Pyspark: Dataframe Checkpoint\n",
            "34. 38. Databricks | Pyspark | Interview Question | Compression Methods: Snappy vs Gzip\n",
            "35. 39. Databricks | Spark | Pyspark Functions| Split\n",
            "36. 40. Databricks | Spark | Pyspark Functions| Arrays_zip\n",
            "37. 41. Databricks | Spark | Pyspark Functions| Part 2 : Array_Intersect\n",
            "38. 42. Databricks | Spark | Pyspark Functions| Part 3 : Array_Except\n",
            "39. 43. Databricks | Spark | Pyspark Functions| Part 4 : Array_Sort\n",
            "40. 44. Databricks | Spark | Python Functions| Join\n",
            "41. 45. Databricks | Spark | Pyspark | PartitionBy\n",
            "42. 46. Databricks | Spark | Pyspark | Number of Records per Partition in Dataframe\n",
            "43. 47. Databricks | Spark | Pyspark | Null Count of Each Column in Dataframe\n",
            "44. 48. Databricks - Pyspark: Find Top or Bottom N Rows per Group\n",
            "45. 49. Databricks & Spark: Interview Question(Scenario Based) - How many spark jobs get created?\n",
            "46. 50. Databricks | Pyspark: Greatest vs Least vs Max vs Min\n",
            "47. 51. Databricks | Pyspark | Delta Lake: Introduction to Delta Lake\n",
            "48. 52. Databricks| Pyspark| Delta Lake Architecture: Internal Working Mechanism\n",
            "49. 53. Databricks| Pyspark| Delta Lake: Solution Architecture\n",
            "50. 54. Databricks | Delta Lake| Pyspark: Create Delta Table Using Various Methods\n",
            "51. 55. Databricks| Pyspark| Delta Lake: Delta Table Instance\n",
            "52. 56. Databricks| Pyspark | Delta Lake: Different Approaches to Insert Data Into Delta Table\n",
            "53. 57. Databricks| Pyspark| Delta Lake: Different Approaches to Delete Data from Delta Table\n",
            "54. 58. Databricks | Pyspark | Delta Lake : Update Delta Table\n",
            "55. 59. Databricks Pyspark:Slowly Changing Dimension|SCD Type1| Merge using Pyspark and Spark SQL\n",
            "56. 60. Databricks & Pyspark: Delta Lake Audit Log Table with Operation Metrics\n",
            "57. 61. Databricks | Pyspark | Delta Lake : Slowly Changing Dimension (SCD Type2)\n",
            "58. 62. Databricks | Pyspark | Delta Lake: Time Travel\n",
            "59. 63. Databricks | Pyspark| Delta Lake: Restore Command\n",
            "60. 64. Databricks | Pyspark | Delta Lake: Optimize Command - File Compaction\n",
            "61. 65. Databricks | Pyspark | Delta Lake: Vacuum Command\n",
            "62. 66. Databricks | Pyspark | Delta: Z-Order Command\n",
            "63. 67. Databricks | Pypark | Delta: Schema Evolution - MergeSchema\n",
            "64. 68. Databricks | Pyspark | Dataframe InsertInto Delta Table\n",
            "65. 69. Databricks | Spark | Pyspark | Data Skewness| Interview Question: SPARK_PARTITION_ID\n",
            "66. 70. Databricks| Pyspark| Input_File_Name: Identify Input File Name of Corrupt Record\n",
            "67. 71. Databricks | Pyspark | Window Functions: Lead and Lag\n",
            "68. 72. Databricks | Pyspark | Interview Question: Explain Plan\n",
            "69. 73. Databricks | Pyspark | UDF to Check if Folder Exists\n",
            "70. 74. Databricks | Pyspark | Interview Question: Sort-Merge Join (SMJ)\n",
            "71. 75. Databricks | Pyspark | Performance Optimization - Bucketing\n",
            "72. 76. Databricks|Pyspark:Interview Question|Scenario Based|Max Over () Get Max value of Duplicate Data\n",
            "73. 77. Databricks | Pyspark | Create_map(): Convert Dataframe Columns to Dictionary (Map Type)\n",
            "74. 78. Databricks | Pyspark | Performance Optimization: Delta Cache\n",
            "75. 79. Databricks | Pyspark | Split Array Elements into Separate Columns\n",
            "76. 80. Databricks | Pyspark | Tips: Write Dataframe into Single File with Specific File Name\n",
            "77. 81. Databricks | Pyspark | Workspace Object Access Control\n",
            "78. 82. Databricks | Pyspark | Databricks Secret Scopes: Azure Key Vault Backed Secrets\n",
            "79. 83. Databricks | Pyspark | Databricks Workflows: Job Scheduling\n",
            "80. 84. Databricks | Pyspark | Azure Data Factory + Azure Databricks: Execute Notebook Via ADF\n",
            "81. 85. Databricks | Pyspark | Notebook Activity in Azure Data Factory with Input Parameter\n",
            "82. 86. Databricks | Pyspark | Notebook Activity in Azure Data Factory with Output Parameter\n",
            "83. 87. Databricks | Pyspark | Real Time Project: ETL Pipeline Integrating ADF, ASQL, ADLS, Key Vault\n",
            "84. 88. Databricks |Pyspark |Notebook Scheduling through Schedule Based Trigger using Azure Data Factory\n",
            "85. 89. Databricks | Pyspark | Notebook Scheduling through Event Based Trigger using Azure Data Factory\n",
            "86. 90. Databricks | Pyspark | Interview Question: Read Excel File with Multiple Sheets\n",
            "87. 91. Databricks | Pyspark | Interview Question |Handlining Duplicate Data: DropDuplicates vs Distinct\n",
            "88. 92. Databricks | Pyspark | Interview Question | Performance Optimization: Select vs WithColumn\n",
            "89. 93. Databricks | Pyspark | Interview Question | Schema Definition: Struct Type vs Struct Field\n",
            "90. 94. Databricks | Pyspark | Interview Question | Schema Definition: Struct Type vs Map Type\n",
            "91. 95. Databricks | Pyspark | Schema | Different Methods of Schema Definition\n",
            "92. 96. Databricks | Pyspark | Real Time Scenario | Schema Comparison\n",
            "93. 97. Databricks | Pyspark | Data Security: Enforcing Column Level Encryption\n",
            "94. 98. Databricks | Pyspark | Interview Question: Pyspark VS Pandas\n",
            "95. 99. Databricks | Pyspark | Real Time Use Case: Generate Test Data - Array_Repeat()\n",
            "96. 100. Databricks | Pyspark | Spark Architecture: Internals of Partition Creation Demystified\n",
            "97. 106.Databricks|Pyspark|Automation|Real Time Project:DataType Issue When Writing to Azure Synapse/SQL\n",
            "98. 107. Databricks | Pyspark| Transformation: Subtract vs ExceptAll\n",
            "99. 108. Databricks | Pyspark| Window Function: First and Last\n",
            "100. 112. Databricks | Pyspark| Spark Reader: Skip First N Records While Reading CSV File\n",
            "101. 113. Databricks | PySpark| Spark Reader: Skip Specific Range of Records While Reading CSV File\n",
            "102. 116. Databricks | Pyspark| Query Dataframe Using Spark SQL\n",
            "103. 119. Databricks | Pyspark| Spark SQL: Except Columns in Select Clause\n",
            "104. 121. Databricks | Pyspark| AutoLoader: Incremental Data Load\n",
            "105. 122. Databricks | Pyspark| Delta Live Table: Introduction\n",
            "106. 123. Databricks | Pyspark| Delta Live Table: Declarative VS Procedural\n",
            "107. 124. Databricks | Pyspark| Delta Live Table: Datasets - Tables and Views\n",
            "108. 125. Databricks | Pyspark| Delta Live Table: Data Quality Check - Expect\n",
            "109. 126. Databricks | Pyspark | Downloading Files from Databricks DBFS Location\n",
            "110. 128. Databricks | Pyspark| Built-In Function: TRANSFORM\n",
            "111. 129. Databricks | Pyspark| Delta Lake: Deletion Vectors\n",
            "112. 130. Databricks | Pyspark| Delta Lake: Change Data Feed\n",
            "113. 131. Databricks | Pyspark| Built-in Function: ZIP_WITH\n",
            "114. 132: DataBricks Learning: System Variable  _SQLDF\n",
            "\n",
            "Do you want to save these titles to a text file? (yes/no): yes\n",
            "Enter the filename (e.g., my_playlist_titles.txt): SparkTopics\n",
            "Titles saved to 'SparkTopics' successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2Lw3d3g3Xc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}